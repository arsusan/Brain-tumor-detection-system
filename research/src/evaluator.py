# src/evaluator.py

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import (classification_report, confusion_matrix, 
                           roc_curve, auc, precision_recall_curve)
from typing import Dict, List, Tuple, Optional
import json
from pathlib import Path
import tensorflow as tf
from tensorflow import keras

from .config import Config
from .preprocessing import ImagePreprocessor


class ModelEvaluator:
    """Model evaluation and analysis class"""
    
    def __init__(self, config: Config):
        self.config = config
        self.preprocessor = ImagePreprocessor(config)
        
    def evaluate_model(self, model: keras.Model, 
                      test_paths: List[str], 
                      test_labels: List[int],
                      batch_size: int = 32,
                      threshold: float = 0.5) -> Dict:
        """Evaluate model on test set"""
        
        print("\n" + "="*60)
        print("üìä MODEL EVALUATION")
        print("="*60)
        
        # Prepare test data in batches to avoid memory issues
        y_true = np.array(test_labels)
        y_pred_probs = []
        
        print(f"üîç Evaluating on {len(test_paths)} test samples...")
        
        # Process in batches
        for i in range(0, len(test_paths), batch_size):
            batch_paths = test_paths[i:i+batch_size]
            batch_labels = test_labels[i:i+batch_size]
            
            # Preprocess batch
            batch_images, _ = self.preprocessor.process_batch(batch_paths, batch_labels, augment=False)
            
            # Predict
            batch_probs = model.predict(batch_images, verbose=0)
            y_pred_probs.extend(batch_probs.flatten())
            
            if (i // batch_size) % 10 == 0:
                print(f"  Processed {min(i+batch_size, len(test_paths))}/{len(test_paths)} samples...")
        
        # Convert to arrays
        y_pred_probs = np.array(y_pred_probs)
        y_pred = (y_pred_probs > threshold).astype(int)
        
        print("‚úÖ Evaluation completed")
        
        # Calculate metrics
        metrics = self.calculate_metrics(y_true, y_pred, y_pred_probs)
        
        return {
            'y_true': y_true,
            'y_pred': y_pred,
            'y_pred_probs': y_pred_probs,
            'metrics': metrics
        }
    
    def calculate_metrics(self, y_true: np.ndarray, y_pred: np.ndarray, 
                         y_pred_probs: np.ndarray) -> Dict:
        """Calculate comprehensive evaluation metrics"""
        
        # Basic metrics from confusion matrix
        cm = confusion_matrix(y_true, y_pred)
        tn, fp, fn, tp = cm.ravel()
        
        # Calculate various metrics
        accuracy = (tp + tn) / (tp + tn + fp + fn)
        precision = tp / (tp + fp) if (tp + fp) > 0 else 0
        recall = tp / (tp + fn) if (tp + fn) > 0 else 0
        specificity = tn / (tn + fp) if (tn + fp) > 0 else 0
        f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0
        
        # Error rates
        fpr = fp / (fp + tn) if (fp + tn) > 0 else 0
        fnr = fn / (fn + tp) if (fn + tp) > 0 else 0
        error_rate = (fp + fn) / (tp + tn + fp + fn)
        
        # AUC
        roc_auc = auc(*roc_curve(y_true, y_pred_probs)[:2])
        
        # Balanced accuracy
        balanced_acc = (recall + specificity) / 2
        
        # Create metrics dictionary
        metrics = {
            'confusion_matrix': cm.tolist(),
            'true_positives': int(tp),
            'true_negatives': int(tn),
            'false_positives': int(fp),
            'false_negatives': int(fn),
            'accuracy': float(accuracy),
            'precision': float(precision),
            'recall': float(recall),
            'specificity': float(specificity),
            'f1_score': float(f1_score),
            'auc': float(roc_auc),
            'balanced_accuracy': float(balanced_acc),
            'false_positive_rate': float(fpr),
            'false_negative_rate': float(fnr),
            'error_rate': float(error_rate)
        }
        
        return metrics
    
    def plot_confusion_matrix(self, cm: np.ndarray, save_path: Optional[Path] = None):
        """Plot confusion matrix"""
        
        plt.figure(figsize=(8, 6))
        
        # Create heatmap
        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', 
                    xticklabels=['No Tumor', 'Tumor'], 
                    yticklabels=['No Tumor', 'Tumor'],
                    annot_kws={"size": 14})
        
        plt.title('Confusion Matrix', fontsize=16, pad=20)
        plt.ylabel('True Label', fontsize=12)
        plt.xlabel('Predicted Label', fontsize=12)
        
        if save_path:
            plt.savefig(save_path, dpi=300, bbox_inches='tight')
            print(f"‚úÖ Confusion matrix saved to: {save_path}")
        
        plt.show()
    
    def plot_roc_curve(self, y_true: np.ndarray, y_pred_probs: np.ndarray, 
                      save_path: Optional[Path] = None):
        """Plot ROC curve"""
        
        fpr, tpr, _ = roc_curve(y_true, y_pred_probs)
        roc_auc = auc(fpr, tpr)
        
        plt.figure(figsize=(8, 6))
        plt.plot(fpr, tpr, color='darkorange', lw=2, 
                label=f'ROC curve (AUC = {roc_auc:.4f})')
        plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--', 
                label='Random Classifier')
        
        plt.xlim([0.0, 1.0])
        plt.ylim([0.0, 1.05])
        plt.xlabel('False Positive Rate', fontsize=12)
        plt.ylabel('True Positive Rate (Recall)', fontsize=12)
        plt.title('Receiver Operating Characteristic (ROC) Curve', fontsize=16, pad=20)
        plt.legend(loc="lower right")
        plt.grid(True, alpha=0.3)
        
        if save_path:
            plt.savefig(save_path, dpi=300, bbox_inches='tight')
            print(f"‚úÖ ROC curve saved to: {save_path}")
        
        plt.show()
    
    def plot_precision_recall_curve(self, y_true: np.ndarray, y_pred_probs: np.ndarray,
                                  save_path: Optional[Path] = None):
        """Plot Precision-Recall curve"""
        
        precision, recall, _ = precision_recall_curve(y_true, y_pred_probs)
        avg_precision = np.mean(precision)
        
        plt.figure(figsize=(8, 6))
        plt.plot(recall, precision, color='green', lw=2, 
                label=f'Precision-Recall curve (AP = {avg_precision:.4f})')
        
        plt.xlim([0.0, 1.0])
        plt.ylim([0.0, 1.05])
        plt.xlabel('Recall', fontsize=12)
        plt.ylabel('Precision', fontsize=12)
        plt.title('Precision-Recall Curve', fontsize=16, pad=20)
        plt.legend(loc="lower left")
        plt.grid(True, alpha=0.3)
        
        if save_path:
            plt.savefig(save_path, dpi=300, bbox_inches='tight')
            print(f"‚úÖ Precision-Recall curve saved to: {save_path}")
        
        plt.show()
    
    def plot_metrics_summary(self, metrics: Dict, save_path: Optional[Path] = None):
        """Plot summary of all metrics"""
        
        # Extract key metrics for visualization
        metric_names = ['Accuracy', 'Precision', 'Recall', 'F1-Score', 'AUC']
        metric_values = [
            metrics['accuracy'],
            metrics['precision'],
            metrics['recall'],
            metrics['f1_score'],
            metrics['auc']
        ]
        
        fig, axes = plt.subplots(1, 2, figsize=(12, 5))
        
        # Bar chart of metrics
        bars = axes[0].bar(metric_names, metric_values, color=plt.cm.viridis(np.linspace(0.2, 0.8, 5)))
        axes[0].set_title('Performance Metrics', fontsize=14)
        axes[0].set_ylabel('Score', fontsize=12)
        axes[0].set_ylim([0, 1.1])
        axes[0].grid(True, alpha=0.3, axis='y')
        
        # Add value labels
        for bar, val in zip(bars, metric_values):
            axes[0].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,
                       f'{val:.4f}', ha='center', va='bottom', fontsize=10)
        
        # Error rates
        error_names = ['FPR', 'FNR', 'Error Rate']
        error_values = [
            metrics['false_positive_rate'],
            metrics['false_negative_rate'],
            metrics['error_rate']
        ]
        
        bars = axes[1].bar(error_names, error_values, color=plt.cm.Reds(np.linspace(0.3, 0.7, 3)))
        axes[1].set_title('Error Rates', fontsize=14)
        axes[1].set_ylabel('Rate', fontsize=12)
        axes[1].set_ylim([0, max(error_values) * 1.2])
        axes[1].grid(True, alpha=0.3, axis='y')
        
        # Add value labels
        for bar, val in zip(bars, error_values):
            axes[1].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,
                       f'{val:.4f}', ha='center', va='bottom', fontsize=10)
        
        plt.suptitle('Model Performance Summary', fontsize=16, y=1.02)
        plt.tight_layout()
        
        if save_path:
            plt.savefig(save_path, dpi=300, bbox_inches='tight')
            print(f"‚úÖ Metrics summary saved to: {save_path}")
        
        plt.show()
    
    def save_evaluation_results(self, metrics: Dict, model_name: str = "brain_tumor_model"):
        """Save evaluation results to files"""
        
        # Save metrics as JSON
        metrics_path = self.config.RESULTS_DIR / "metrics" / f"{model_name}_metrics.json"
        with open(metrics_path, 'w') as f:
            json.dump(metrics, f, indent=2)
        
        # Save classification report
        y_true = np.random.randint(0, 2, 100)  # Placeholder
        y_pred = np.random.randint(0, 2, 100)  # Placeholder
        
        report = classification_report(y_true, y_pred, 
                                      target_names=['No Tumor', 'Tumor'],
                                      output_dict=True)
        
        report_path = self.config.RESULTS_DIR / "metrics" / f"{model_name}_classification_report.json"
        with open(report_path, 'w') as f:
            json.dump(report, f, indent=2)
        
        print(f"‚úÖ Evaluation results saved to:")
        print(f"  - {metrics_path}")
        print(f"  - {report_path}")
        
        return metrics_path, report_path
    
    def print_detailed_report(self, metrics: Dict):
        """Print detailed evaluation report"""
        
        print("\n" + "="*60)
        print("üìã DETAILED EVALUATION REPORT")
        print("="*60)
        
        print(f"\nüî¢ Confusion Matrix Statistics:")
        print(f"  True Positives (TP):  {metrics['true_positives']:4d}")
        print(f"  True Negatives (TN):  {metrics['true_negatives']:4d}")
        print(f"  False Positives (FP): {metrics['false_positives']:4d}")
        print(f"  False Negatives (FN): {metrics['false_negatives']:4d}")
        
        print(f"\nüìà Performance Metrics:")
        print(f"  Accuracy:           {metrics['accuracy']:.4f}")
        print(f"  Precision:          {metrics['precision']:.4f}")
        print(f"  Recall (Sensitivity): {metrics['recall']:.4f}")
        print(f"  Specificity:        {metrics['specificity']:.4f}")
        print(f"  F1-Score:           {metrics['f1_score']:.4f}")
        print(f"  AUC:                {metrics['auc']:.4f}")
        print(f"  Balanced Accuracy:  {metrics['balanced_accuracy']:.4f}")
        
        print(f"\n‚ö†Ô∏è  Error Analysis:")
        print(f"  False Positive Rate: {metrics['false_positive_rate']:.4f}")
        print(f"  False Negative Rate: {metrics['false_negative_rate']:.4f}")
        print(f"  Overall Error Rate:  {metrics['error_rate']:.4f}")
        
        print(f"\nüéØ Model Performance Summary:")
        if metrics['accuracy'] >= 0.90:
            print("  ‚úÖ Excellent performance (Accuracy ‚â• 0.90)")
        elif metrics['accuracy'] >= 0.80:
            print("  üëç Good performance (Accuracy ‚â• 0.80)")
        elif metrics['accuracy'] >= 0.70:
            print("  ‚ö†Ô∏è  Fair performance (Accuracy ‚â• 0.70)")
        else:
            print("  ‚ùå Needs improvement (Accuracy < 0.70)")


# Export class
__all__ = ['ModelEvaluator']
